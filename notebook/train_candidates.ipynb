{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "143e40de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use RAPIDS version 22.10.01+2.gca9a422da9\n"
     ]
    }
   ],
   "source": [
    "VER = 6\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os, sys, pickle, glob, gc\n",
    "from collections import Counter\n",
    "import cudf, itertools\n",
    "print('We will use RAPIDS version',cudf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69ca54b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance of type weighting タイプごとの重み付けバランス\n",
    "# 0:clicks 1:carts 2:orders\n",
    "type_weight = {0:0.5,\n",
    "               1:9,\n",
    "               2:0.5}\n",
    "type_weight_multipliers = type_weight\n",
    "\n",
    "# Use top X for clicks, carts and orders Top何位までを使うか\n",
    "clicks_th = 15 # クリック数\n",
    "carts_th  = 20 # カート数\n",
    "orders_th = 20 # 購入数\n",
    "\n",
    "VER = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf8206ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CANDIDATES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3228a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# CACHE FUNCTIONS\n",
    "def read_file(f):\n",
    "    return cudf.DataFrame( data_cache[f] )\n",
    "def read_file_to_cache(f):\n",
    "    df = pd.read_parquet(f)\n",
    "    df.ts = (df.ts/1000).astype('int32')\n",
    "    df['type'] = df['type'].map(type_labels).astype('int8')\n",
    "    return df\n",
    "\n",
    "# CACHE THE DATA ON CPU BEFORE PROCESSING ON GPU\n",
    "data_cache = {}\n",
    "type_labels = {'clicks':0, 'carts':1, 'orders':2}\n",
    "files = glob.glob('../data/input/otto-validation/*_parquet/*')\n",
    "for f in files: data_cache[f] = read_file_to_cache(f)\n",
    "\n",
    "# CHUNK PARAMETERS\n",
    "READ_CT = 1\n",
    "CHUNK = int( np.ceil( len(files)/6 ))\n",
    "print(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9423c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CARTS_ORDERS_DISK_PIECES = 10\n",
    "BUY2BUY_DISK_PIECES = 4\n",
    "CLICKS_DISK_PIECES = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce043a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 10\n",
    "CARTS_ORDERS_DISK_PIECES = DISK_PIECES\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            \n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            \n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "            \n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            \n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', 'type_y'])\n",
    "            df['wgt'] = df.type_y.map(type_weight)\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            \n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    \n",
    "    # SAVE TOP 40\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<carts_th].drop('n',axis=1)\n",
    "    \n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'top_15_carts_orders_v{VER}_{PART}.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf48432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 4\n",
    "BUY2BUY_DISK_PIECES = DISK_PIECES\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            \n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.loc[df['type'].isin([1,2])] # ONLY WANT CARTS AND ORDERS\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            \n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            \n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ] # 14 DAYS\n",
    "            \n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            \n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', 'type_y'])\n",
    "            df['wgt'] = 1\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            \n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "\n",
    "        print()\n",
    "        \n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    \n",
    "    # SAVE TOP 40\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<orders_th].drop('n',axis=1)\n",
    "    \n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'top_15_buy2buy_v{VER}_{PART}.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885a570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 8\n",
    "CLICKS_DISK_PIECES = DISK_PIECES\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            \n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            \n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "            \n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            \n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n",
    "            # 1659304800 : minimum timestamp\n",
    "            # 1662328791 : maximum timestamp\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            \n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "        print()\n",
    "        \n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    \n",
    "    # SAVE TOP 40\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<clicks_th].drop('n',axis=1)\n",
    "    \n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'top_20_clicks_v{VER}_{PART}.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd38eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CARTS_ORDERS_DISK_PIECES = 10\n",
    "BUY2BUY_DISK_PIECES = 4\n",
    "CLICKS_DISK_PIECES = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f777ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREE MEMORY\n",
    "del data_cache, tmp\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913e2d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test():    \n",
    "    dfs = []\n",
    "    for e, chunk_file in enumerate(glob.glob('../data/input/otto-validation/test_parquet/*')):\n",
    "        chunk = pd.read_parquet(chunk_file)\n",
    "        chunk.ts = (chunk.ts/1000).astype('int32')\n",
    "        chunk['type'] = chunk['type'].map(type_labels).astype('int8')\n",
    "        dfs.append(chunk)\n",
    "    return pd.concat(dfs).reset_index(drop=True) #.astype({\"ts\": \"datetime64[ms]\"})\n",
    "\n",
    "test_df = load_test()\n",
    "print('Test data has shape',test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef09c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# LOAD THREE CO-VISITATION MATRICES\n",
    "def pqt_to_dict(df):\n",
    "    return df.groupby('aid_x').aid_y.apply(list).to_dict()\n",
    "\n",
    "top_20_clicks = pqt_to_dict( pd.read_parquet(f'top_20_clicks_v{VER}_0.pqt') )\n",
    "for k in range(1,CLICKS_DISK_PIECES): \n",
    "    top_20_clicks.update( pqt_to_dict( pd.read_parquet(f'top_20_clicks_v{VER}_{k}.pqt') ) )\n",
    "top_20_buys = pqt_to_dict( pd.read_parquet(f'top_15_carts_orders_v{VER}_0.pqt') )\n",
    "for k in range(1,CARTS_ORDERS_DISK_PIECES): \n",
    "    top_20_buys.update( pqt_to_dict( pd.read_parquet(f'top_15_carts_orders_v{VER}_{k}.pqt') ) )\n",
    "top_20_buy2buy = pqt_to_dict( pd.read_parquet(f'top_15_buy2buy_v{VER}_0.pqt') )\n",
    "for k in range(1,BUY2BUY_DISK_PIECES): \n",
    "    top_20_buy2buy.update( pqt_to_dict( pd.read_parquet(f'top_15_buy2buy_v{VER}_{k}.pqt') ) )\n",
    "\n",
    "# TOP CLICKS AND ORDERS IN TEST\n",
    "# top_clicks = test_df.loc[test_df['type']==type_labels['clicks'],'aid'].value_counts().index.values[:N_CANDIDATES]\n",
    "# top_orders = test_df.loc[test_df['type']==type_labels['orders'],'aid'].value_counts().index.values[:N_CANDIDATES]\n",
    "\n",
    "print('Here are size of our 3 co-visitation matrices:')\n",
    "print( len( top_20_clicks ), len( top_20_buy2buy ), len( top_20_buys ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac44067",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_clicks = test_df.loc[test_df['type']== 0,'aid'].value_counts().index.values[:N_CANDIDATES]\n",
    "top_carts = test_df.loc[test_df['type']== 1,'aid'].value_counts().index.values[:N_CANDIDATES]\n",
    "top_orders = test_df.loc[test_df['type']== 2,'aid'].value_counts().index.values[:N_CANDIDATES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73c34f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_clicks(df):\n",
    "    # USER HISTORY AIDS AND TYPES\n",
    "    aids=df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1] ))\n",
    "    # RERANK CANDIDATES USING WEIGHTS\n",
    "    if len(unique_aids)>=N_CANDIDATES:\n",
    "        weights=np.logspace(0.1,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter() \n",
    "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        sorted_aids = [k for k,v in aids_temp.most_common(N_CANDIDATES)]\n",
    "        return sorted_aids\n",
    "    # USE \"CLICKS\" CO-VISITATION MATRIX\n",
    "    weights=np.logspace(0.1,1,len(aids),base=2, endpoint=True)-1\n",
    "    aids_temp = Counter() \n",
    "    # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "    for aid,w,t in zip(aids,weights,types): \n",
    "        aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "    sorted_aids = [k for k,v in aids_temp.most_common(len(unique_aids))]\n",
    "    # HACK ABOVE\n",
    "    aids2 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2).most_common(N_CANDIDATES) if aid2 not in unique_aids]    \n",
    "    result = sorted_aids + top_aids2[:N_CANDIDATES - len(unique_aids)]\n",
    "    # USE TOP20 TEST CLICKS\n",
    "#     return result + list(top_clicks)[:N_CANDIDATES-len(result)]\n",
    "    set_result = set(result)\n",
    "    return result + [i for i in list(top_clicks) if i not in set_result][:N_CANDIDATES - len(result)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd79a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_carts(df):\n",
    "    # User history aids and types\n",
    "    aids = df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    \n",
    "    # UNIQUE AIDS AND UNIQUE BUYS\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1] ))\n",
    "    df = df.loc[(df['type'] == 0)|(df['type'] == 1)]\n",
    "    unique_buys = list(dict.fromkeys(df.aid.tolist()[::-1]))\n",
    "    \n",
    "    # Rerank candidates using weights\n",
    "    if len(unique_aids) >= N_CANDIDATES:\n",
    "        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter() \n",
    "        \n",
    "        # Rerank based on repeat items and types of items\n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        \n",
    "        # Rerank candidates using\"top_20_carts\" co-visitation matrix\n",
    "        aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_buys if aid in top_20_buys]))\n",
    "        for aid in aids2: aids_temp[aid] += 0.1\n",
    "        sorted_aids = [k for k,v in aids_temp.most_common(N_CANDIDATES)]\n",
    "        return sorted_aids\n",
    "    \n",
    "    # HACK\n",
    "    weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n",
    "    aids_temp = Counter() \n",
    "\n",
    "    # Rerank based on repeat items and types of items\n",
    "    for aid,w,t in zip(aids,weights,types): \n",
    "        aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "\n",
    "    # Rerank candidates using\"top_20_carts\" co-visitation matrix\n",
    "    aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_buys if aid in top_20_buys]))\n",
    "    for aid in aids2: aids_temp[aid] += 0.1\n",
    "    sorted_aids = [k for k,v in aids_temp.most_common(len(unique_aids))]\n",
    "    \n",
    "    # Use \"cart order\" and \"clicks\" co-visitation matrices\n",
    "    aids1 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n",
    "    aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_aids if aid in top_20_buys]))\n",
    "    \n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids1+aids2).most_common(N_CANDIDATES) if aid2 not in unique_aids] \n",
    "    result = sorted_aids + top_aids2[:N_CANDIDATES - len(unique_aids)]\n",
    "    \n",
    "    # USE TOP20 TEST ORDERS\n",
    "#     return result + list(top_carts)[:N_CANDIDATES-len(result)]\n",
    "    set_result = set(result)\n",
    "    return result + [i for i in list(top_carts) if i not in set_result][:N_CANDIDATES - len(result)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afa20bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_buys(df):\n",
    "    # USER HISTORY AIDS AND TYPES\n",
    "    aids=df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    # UNIQUE AIDS AND UNIQUE BUYS\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1] ))\n",
    "    df = df.loc[(df['type']==1)|(df['type']==2)]\n",
    "    unique_buys = list(dict.fromkeys( df.aid.tolist()[::-1] ))\n",
    "    # RERANK CANDIDATES USING WEIGHTS\n",
    "    if len(unique_aids)>=N_CANDIDATES:\n",
    "        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter() \n",
    "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        # RERANK CANDIDATES USING \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "        aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n",
    "        for aid in aids3: aids_temp[aid] += 0.1\n",
    "        sorted_aids = [k for k,v in aids_temp.most_common(N_CANDIDATES)]\n",
    "        return sorted_aids\n",
    "    # HACK\n",
    "    weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n",
    "    aids_temp = Counter() \n",
    "    # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "    for aid,w,t in zip(aids,weights,types): \n",
    "        aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "    # RERANK CANDIDATES USING \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "    aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n",
    "    for aid in aids3: aids_temp[aid] += 0.1\n",
    "    sorted_aids = [k for k,v in aids_temp.most_common(len(unique_aids))]\n",
    "    # USE \"CART ORDER\" CO-VISITATION MATRIX\n",
    "    aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_aids if aid in top_20_buys]))\n",
    "    # USE \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "    aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2+aids3).most_common(N_CANDIDATES) if aid2 not in unique_aids] \n",
    "    result = sorted_aids + top_aids2[:N_CANDIDATES - len(unique_aids)]\n",
    "    # USE TOP20 TEST ORDERS\n",
    "#     return result + list(top_orders)[:N_CANDIDATES-len(result)]\n",
    "    set_result = set(result)\n",
    "    return result + [i for i in list(top_orders) if i not in set_result][:N_CANDIDATES - len(result)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aaa067",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pred_df = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n",
    "    lambda x: suggest_clicks(x)\n",
    ")\n",
    "pd.DataFrame(pred_df).rename({0:\"aid\"}, axis=1).explode(\"aid\").reset_index().to_parquet(\n",
    "    f\"/home/search2/lichunyu/otto-recommender-system/data/input/otto-candidates/valid_a_candidates/valid_a_clicks_candidates_top{N_CANDIDATES}.parquet\"\n",
    ")\n",
    "\n",
    "pred_df = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n",
    "    lambda x: suggest_carts(x)\n",
    ")\n",
    "pd.DataFrame(pred_df).rename({0:\"aid\"}, axis=1).explode(\"aid\").reset_index().to_parquet(\n",
    "    f\"/home/search2/lichunyu/otto-recommender-system/data/input/otto-candidates/valid_a_candidates/valid_a_carts_candidates_top{N_CANDIDATES}.parquet\"\n",
    ")\n",
    "\n",
    "pred_df = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n",
    "    lambda x: suggest_buys(x)\n",
    ")\n",
    "pd.DataFrame(pred_df).rename({0:\"aid\"}, axis=1).explode(\"aid\").reset_index().to_parquet(\n",
    "    f\"/home/search2/lichunyu/otto-recommender-system/data/input/otto-candidates/valid_a_candidates/valid_a_orders_candidates_top{N_CANDIDATES}.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2d43c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2c80e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a4dff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfde9bec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a2f6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff931f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf1f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0404067e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec1f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ad6d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4a5813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92187fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45369b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1580c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pd.DataFrame(pred_df_clicks).rename({0:\"aid\"}, axis=1).explode(\"aid\").reset_index().to_parquet(\n",
    "    f\"/home/search2/lichunyu/otto-recommender-system/data/input/otto-candidates/valid_a_candidates/valid_a_clicks_candidates_top{N_CANDIDATES}.parquet\"\n",
    ")\n",
    "pd.DataFrame(pred_df_carts).rename({0:\"aid\"}, axis=1).explode(\"aid\").reset_index().to_parquet(\n",
    "    f\"/home/search2/lichunyu/otto-recommender-system/data/input/otto-candidates/valid_a_candidates/valid_a_carts_candidates_top{N_CANDIDATES}.parquet\"\n",
    ")\n",
    "pd.DataFrame(pred_df_buys).rename({0:\"aid\"}, axis=1).explode(\"aid\").reset_index().to_parquet(\n",
    "    f\"/home/search2/lichunyu/otto-recommender-system/data/input/otto-candidates/valid_a_candidates/valid_a_orders_candidates_top{N_CANDIDATES}.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812c8747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005308de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
