{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb287cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import gc\n",
    "import warnings\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "# import cudf\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import joblib\n",
    "# from openfe import openfe, transform\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9491800c",
   "metadata": {},
   "source": [
    "# Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd2d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/search3/lichunyu/otto-recommender-system/data\"\n",
    "TYPE_MAP = {'clicks':0, 'carts':1, 'orders':2}\n",
    "TOPN = 60\n",
    "DOWNSAMPLE_RATE = 20\n",
    "VALID_DATA_RATIO = 0.2\n",
    "\n",
    "def read_parquet(f):\n",
    "    df = pd.read_parquet(f)\n",
    "    df.ts = (df.ts/1000).astype('int32')\n",
    "    df['type'] = df['type'].map(TYPE_MAP).astype('int8')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f9dd5",
   "metadata": {},
   "source": [
    "# Fill NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46afa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_default(df):\n",
    "    df = df.fillna({\n",
    "        \"user_first_clicks_time\": 8.0,\n",
    "        \"user_last_clicks_time\": 8.0,\n",
    "        \"user_first_orders_time\": 8.0,\n",
    "        \"user_last_orders_time\": 8.0,\n",
    "        \"user_first_carts_time\": 8.0,\n",
    "        \"user_last_carts_time\": 8.0,\n",
    "        \"interaction_type_core\": 0,\n",
    "        \"item_last_clicks_ts\": 30,\n",
    "        \"item_first_clicks_ts\": 30,\n",
    "        \"item_last_carts_ts\": 30,\n",
    "        \"item_first_carts_ts\": 30,\n",
    "        \"item_last_orders_ts\": 30,\n",
    "        \"item_first_orders_ts\": 30,\n",
    "        \"interaction_clicks_carts_ratio\": 0,\n",
    "        \"interaction_carts_clicks_ratio\": 0,\n",
    "        \"interaction_clicks_orders_ratio\": 0,\n",
    "        \"interaction_orders_clicks_ratio\": 0,\n",
    "        \"interaction_orders_carts_ratio\": 0,\n",
    "        \"interaction_carts_orders_ratio\": 0,\n",
    "    }, axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bd7152",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668ec930",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# valid_a_candidates_path = sorted(glob.glob(\n",
    "#     os.path.join(DATA_PATH, \"input/otto-candidates/valid_a_candidates/valid_a_candidates_buys_top50.parquet \")\n",
    "# ))\n",
    "\n",
    "# df_train_candidates = pd.concat([pd.read_parquet(i) for i in valid_a_candidates_path])\n",
    "\n",
    "# valid_a_candidates_path = os.path.join(DATA_PATH, \"input/otto-candidates/valid_a_candidates/valid_a_orders_candidates_top50.parquet\")\n",
    "valid_a_candidates_path = os.path.join(DATA_PATH, f\"input/otto-candidates/valid_a_candidates/valid_a_orders_candidates_top{TOPN}.parquet\")\n",
    "df_train_candidates = pd.read_parquet(valid_a_candidates_path)\n",
    "\n",
    "\n",
    "df_train_label = pd.read_parquet(\n",
    "    os.path.join(DATA_PATH, \"input/otto-validation/test_labels.parquet\")\n",
    ")\n",
    "# df_train_label = df_train_label[(df_train_label[\"type\"]==\"orders\")|(df_train_label[\"type\"]==\"carts\")][[\"session\", \"ground_truth\"]] # TODO test it\n",
    "df_train_label = df_train_label[(df_train_label[\"type\"]==\"orders\")][[\"session\", \"ground_truth\"]]\n",
    "df_train_label = df_train_label.explode(\"ground_truth\").reset_index(drop=True)\n",
    "df_train_label = df_train_label.rename({\"ground_truth\": \"aid\"}, axis=1)\n",
    "df_train_label[\"label\"] = 1\n",
    "\n",
    "# df_train_candidates = df_train_candidates.merge(df_train_label, on=[\"session\", \"aid\"], how=\"outer\").fillna({\"label\": 0})\n",
    "\n",
    "df_train_candidates = df_train_candidates.merge(df_train_label, on=[\"session\", \"aid\"], how=\"left\").fillna({\"label\": 0})\n",
    "df_train_candidates[\"label\"] = df_train_candidates[\"label\"].astype(\"int\")\n",
    "df_train_candidates[\"recall_order\"] = df_train_candidates.groupby('session').cumcount()\n",
    "df_train_candidates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9331182c",
   "metadata": {},
   "source": [
    "## Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c433a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def downsample(df, n=-1):\n",
    "    if n == -1:\n",
    "        n = DOWNSAMPLE_RATE\n",
    "    df_negative = df[df[\"label\"]==0]\n",
    "    df_postive = df[df[\"label\"]==1]\n",
    "    r = len(df_negative)//len(df_postive)\n",
    "    print(f\"current negative size: {len(df_negative)}, postive size: {len(df_postive)}, rate: {r}\")\n",
    "    if r > n:\n",
    "        gloden_negative_size = n * len(df_postive)\n",
    "        df_negative = df_negative.sample(gloden_negative_size)\n",
    "        df = pd.concat([df_postive, df_negative])\n",
    "    df[\"_noise\"] = np.random.randn(len(df))\n",
    "    df = df.sort_values([\"session\", \"_noise\"])\n",
    "    df = df.drop(\"_noise\", axis=1).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_valid_split(df):\n",
    "    print(f\"origin all data size: {len(df)}\")\n",
    "    valid_session = np.random.choice(df_train_candidates.session.unique(), int(len(df_train_candidates.session.unique())*VALID_DATA_RATIO))\n",
    "    df_train = df[~df[\"session\"].isin(valid_session)]\n",
    "    df_valid = df[df[\"session\"].isin(valid_session)]\n",
    "    df_train = downsample(df_train)\n",
    "    return df_train, df_valid\n",
    "\n",
    "\n",
    "df_train_candidates, df_valid_candidates = train_valid_split(df_train_candidates)\n",
    "df_train_candidates[\"label\"] = df_train_candidates[\"label\"].astype(\"int\")\n",
    "df_valid_candidates[\"label\"] = df_valid_candidates[\"label\"].astype(\"int\")\n",
    "print(f\"df_train_candidates size: {len(df_train_candidates)}, df_valid_candidates size: {len(df_valid_candidates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1f73ee",
   "metadata": {},
   "source": [
    "## Merge Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4650e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_item_feature_path = sorted(glob.glob(\n",
    "    os.path.join(DATA_PATH, \"input/feature/train/item_feature_*.parquet\")\n",
    "))\n",
    "for p in tqdm(train_item_feature_path):\n",
    "    df_train_candidates = df_train_candidates.merge(pd.read_parquet(p), on=\"aid\", how=\"left\")\n",
    "    df_valid_candidates = df_valid_candidates.merge(pd.read_parquet(p), on=\"aid\", how=\"left\")\n",
    "\n",
    "\n",
    "train_user_feature_path = sorted(glob.glob(\n",
    "    os.path.join(DATA_PATH, \"input/feature/train/user_feature_*.parquet\")\n",
    "))\n",
    "for p in tqdm(train_user_feature_path):\n",
    "    df_train_candidates = df_train_candidates.merge(pd.read_parquet(p), on=\"session\", how=\"left\")\n",
    "    df_valid_candidates = df_valid_candidates.merge(pd.read_parquet(p), on=\"session\", how=\"left\")\n",
    "\n",
    "\n",
    "train_interaction_feature_path = sorted(glob.glob(\n",
    "    os.path.join(DATA_PATH, \"input/feature/train/interaction_feature_*.parquet\")\n",
    "))\n",
    "for p in tqdm(train_interaction_feature_path):\n",
    "    df_train_candidates = df_train_candidates.merge(pd.read_parquet(p), on=[\"session\", \"aid\"], how=\"left\")\n",
    "    df_valid_candidates = df_valid_candidates.merge(pd.read_parquet(p), on=[\"session\", \"aid\"], how=\"left\")\n",
    "\n",
    "\n",
    "df_train_candidates = fillna_default(df_train_candidates)\n",
    "df_valid_candidates = fillna_default(df_valid_candidates)\n",
    "\n",
    "print(f\"df_train_candidates size: {len(df_train_candidates)}, df_valid_candidates size: {len(df_valid_candidates)}\")\n",
    "df_train_candidates.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a161a7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e9cf0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203df3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6426d06c",
   "metadata": {},
   "source": [
    "## Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcd94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_candidates.to_pickle(\"df_train_candidates.pkl\")\n",
    "df_valid_candidates.to_pickle(\"df_valid_candidates.pkl\")\n",
    "df_train_label[\n",
    "    df_train_label[\"session\"].isin(df_valid_candidates[\"session\"].unique())\n",
    "].reset_index(drop=True).rename({\"aid\": \"ground_truth\"}, axis=1).to_parquet(\"df_valid_label.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddebfec5",
   "metadata": {},
   "source": [
    "## Feature Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ecd0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_COL = list(set(df_train_candidates.columns.tolist()) - set([\"session\", \"aid\", \"ts\", \"label\", \"user_buy_ratio\"]))\n",
    "with open(\"FEATURE_COL.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(FEATURE_COL))\n",
    "\n",
    "print(f\"Count of Feature is: {len(FEATURE_COL)}\")\n",
    "print(\"\")\n",
    "FEATURE_COL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e99efcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b21b99a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ced552b4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aab6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_estimators_candidates = list(range(100,401,50))\n",
    "\n",
    "for n_estimators in tqdm(n_estimators_candidates):\n",
    "    ranker = xgb.XGBRanker(\n",
    "#         tree_method='gpu_hist',\n",
    "        tree_method=\"hist\",\n",
    "        booster='gbtree',\n",
    "        objective='rank:pairwise',\n",
    "        random_state=42, \n",
    "        learning_rate=0.1,\n",
    "        colsample_bytree=0.9,  # 0.9\n",
    "        eta=0.05, \n",
    "        max_depth=6, \n",
    "        n_estimators=n_estimators,\n",
    "        subsample=0.8,\n",
    "        n_jobs=15\n",
    "    )\n",
    "\n",
    "    feature_cols = FEATURE_COL\n",
    "    label_col = 'label'\n",
    "\n",
    "    ranker.fit(\n",
    "        X=df_train_candidates[feature_cols],\n",
    "        y=df_train_candidates[label_col],\n",
    "        group=df_train_candidates.groupby(\"session\").count()[\"label\"]\n",
    "    )\n",
    "\n",
    "    joblib.dump(ranker, f\"models/orders_xgbranker_{str(n_estimators)}.m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2a3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_candidates = list(range(50,501,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd50d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773bbfda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8ee7835",
   "metadata": {},
   "source": [
    "# Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7b2ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# ranker = joblib.load(\"carts_xgbranker_1.m\")\n",
    "\n",
    "# FEATURE_COL = list(ranker.feature_names_in_)\n",
    "with open(\"FEATURE_COL.txt\", \"r\") as f:\n",
    "    FEATURE_COL = f.read().splitlines()\n",
    "    \n",
    "print(FEATURE_COL)\n",
    "\n",
    "for n_estimators in tqdm(n_estimators_candidates[::-1]):\n",
    "    df_valid_candidates = pd.read_pickle(\"df_valid_candidates.pkl\")\n",
    "    df_valid_label = pd.read_parquet(\"df_valid_label.parquet\")\n",
    "    df_valid_label = df_valid_label[[\"session\", \"ground_truth\"]].groupby(\"session\").agg(list).reset_index()\n",
    "    ranker = joblib.load(f\"models/orders_xgbranker_{n_estimators}.m\")\n",
    "    df_valid_candidates[\"score\"] = ranker.predict(df_valid_candidates[FEATURE_COL])\n",
    "    df_valid_candidates = df_valid_candidates.sort_values(by=['session', 'score'], ascending=False)[['session', 'aid']].reset_index(drop=True)\n",
    "    df_valid_candidates = df_valid_candidates.groupby('session').head(20).groupby('session').agg(list).reset_index(drop=False)\n",
    "    df_valid_candidates = df_valid_candidates.merge(df_valid_label, on=\"session\", how=\"left\")\n",
    "    df_valid_candidates[\"ground_truth\"] = df_valid_candidates[\"ground_truth\"].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    df_valid_candidates[\"hits\"] = df_valid_candidates.apply(lambda df: len(set(df.ground_truth).intersection(set(df.aid))), axis=1)\n",
    "    df_valid_candidates['gt_count'] = df_valid_candidates.ground_truth.str.len().clip(0,20)\n",
    "    recall = df_valid_candidates[\"hits\"].sum() / df_valid_candidates['gt_count'].sum()\n",
    "\n",
    "    print(f\"n_estimators={n_estimators} get Recall@20: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24421b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c39d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b5fc43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28213dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2407326",
   "metadata": {},
   "source": [
    "# Chunking kaggle test dataset to save memory(don't execute it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba808a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_kaggle_test = pd.read_parquet(os.path.join(DATA_PATH, \"input/otto-candidates/test_candidates/test_orders_candidates_top50.parquet\"))\n",
    "df_kaggle_test = pd.read_parquet(os.path.join(DATA_PATH, f\"input/otto-candidates/test_candidates/test_orders_candidates_top{TOPN}.parquet\"))\n",
    "\n",
    "inference_session_list = df_kaggle_test[\"session\"].unique().tolist()\n",
    "INFERENCE_CHUNK_SIZE = len(inference_session_list)//INFERENCE_CHUNK_NUM + 2\n",
    "n = 1\n",
    "for i in range(0, len(inference_session_list), INFERENCE_CHUNK_SIZE):\n",
    "    df_kaggle_test[df_kaggle_test[\"session\"].isin(inference_session_list[i:i+INFERENCE_CHUNK_SIZE])].to_parquet(\n",
    "        os.path.join(DATA_PATH, f\"input/otto-candidates/test_candidates/test_orders_candidates_top{TOPN}_chunk_{n}.parquet\")\n",
    "    )\n",
    "    n += 1\n",
    "df_kaggle_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bde2b2",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c38a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE_CHUNK_NUM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fb40f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "INFERENCE_CHUNK_NUM = 10\n",
    "df_kaggle_test = None\n",
    "for n in range(1, INFERENCE_CHUNK_NUM+1):\n",
    "    print(f\"chunk {n} start ...\")\n",
    "    df_kaggle_test_chunk = pd.read_parquet(\n",
    "        os.path.join(DATA_PATH, f\"input/otto-candidates/test_candidates/test_orders_candidates_top{TOPN}_chunk_{n}.parquet\")\n",
    "    )\n",
    "    df_kaggle_test_chunk[\"recall_order\"] = df_kaggle_test_chunk.groupby('session').cumcount()\n",
    "    test_user_feature_path = sorted(glob.glob(\n",
    "        os.path.join(DATA_PATH, \"input/feature/test/user_feature_*.parquet\")\n",
    "    ))\n",
    "    for p in tqdm(test_user_feature_path):\n",
    "        df_kaggle_test_chunk = df_kaggle_test_chunk.merge(pd.read_parquet(p), on=\"session\", how=\"left\")\n",
    "\n",
    "\n",
    "    test_item_feature_path = sorted(glob.glob(\n",
    "        os.path.join(DATA_PATH, \"input/feature/test/item_feature_*.parquet\")\n",
    "    ))\n",
    "    for p in tqdm(test_item_feature_path):\n",
    "        df_kaggle_test_chunk = df_kaggle_test_chunk.merge(pd.read_parquet(p), on=\"aid\", how=\"left\")\n",
    "\n",
    "    test_interaction_feature_path = sorted(glob.glob(\n",
    "        os.path.join(DATA_PATH, \"input/feature/test/interaction_feature_*.parquet\")\n",
    "    ))\n",
    "    for p in tqdm(test_interaction_feature_path):\n",
    "        df_kaggle_test_chunk = df_kaggle_test_chunk.merge(pd.read_parquet(p), on=[\"session\", \"aid\"], how=\"left\")\n",
    "\n",
    "\n",
    "    df_kaggle_test_chunk = fillna_default(df_kaggle_test_chunk)\n",
    "    df_kaggle_test_chunk.to_parquet(f\"df_kaggle_test_chunk_{n}.parquet\")\n",
    "    \n",
    "# df_kaggle_test.to_parquet(\"df_kaggle_test.parquet\")\n",
    "# inference_session_list = df_kaggle_test[\"session\"].unique().tolist()\n",
    "# INFERENCE_CHUNK_NUM = 10\n",
    "# INFERENCE_CHUNK_SIZE = len(inference_session_list)//INFERENCE_CHUNK_NUM + 2\n",
    "# n = 1\n",
    "# for i in range(0, len(inference_session_list), INFERENCE_CHUNK_SIZE):\n",
    "#     df_kaggle_test[df_kaggle_test[\"session\"].isin(inference_session_list[i:INFERENCE_CHUNK_SIZE])].to_parquet(f\"df_kaggle_test_chunk_{n}.parquet\")\n",
    "#     n += 1\n",
    "# df_kaggle_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d251cebe",
   "metadata": {},
   "source": [
    "# Please restart kernel to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7776f6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(\"FEATURE_COL.txt\", \"r\") as f:\n",
    "    FEATURE_COL = f.read().splitlines()\n",
    "print(FEATURE_COL)\n",
    "\n",
    "best_n_estimators = \"200\"\n",
    "ranker = joblib.load(f\"models/orders_xgbranker_{best_n_estimators}.m\")\n",
    "df_kaggle_test = None\n",
    "for n in tqdm(range(1, INFERENCE_CHUNK_NUM+1)):\n",
    "    df = pd.read_parquet(f\"df_kaggle_test_chunk_{n}.parquet\")\n",
    "    df[\"score\"] = ranker.predict(df[FEATURE_COL])\n",
    "    df = df.sort_values(by=['session', 'score'], ascending=False)[['session', 'aid']].reset_index(drop=True)\n",
    "    df = df.groupby('session').head(20).groupby('session').agg(list).reset_index(drop=False)\n",
    "    if df_kaggle_test is None:\n",
    "        df_kaggle_test = df\n",
    "    else:\n",
    "        df_kaggle_test = pd.concat([df_kaggle_test, df])\n",
    "\n",
    "# df_kaggle_test[\"score\"] = ranker.predict(df_kaggle_test[FEATURE_COL])\n",
    "# df_kaggle_test = df_kaggle_test.sort_values(by=['session', 'score'], ascending=False)[['session', 'aid']].reset_index(drop=True)\n",
    "# df_kaggle_test = df_kaggle_test.groupby('session').head(20).groupby('session').agg(list).reset_index(drop=False)\n",
    "df_kaggle_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87bc0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_kaggle_test[\"session_type\"] = df_kaggle_test[\"session\"].apply(lambda x: str(x)+\"_orders\")\n",
    "# df_test[\"session_type\"] = df_test[\"session\"].apply(lambda x: str(x)+\"_carts\")\n",
    "df_kaggle_test = df_kaggle_test.rename({\"aid\": \"labels\"}, axis=1)[[\"session_type\", \"labels\"]]\n",
    "df_kaggle_test[\"labels\"] = df_kaggle_test[\"labels\"].apply(lambda x: \" \".join([str(_) for _ in x]))\n",
    "df_submission = pd.read_csv(\"../data/output/submission_578.csv\")\n",
    "# df_submission = df_submission[~df_submission.session_type.str.contains(\"_carts$\")]\n",
    "df_submission = df_submission[~df_submission.session_type.str.contains(\"_orders$\")]\n",
    "df_submission = pd.concat([df_kaggle_test, df_submission])\n",
    "df_submission.to_csv(\"../data/output/submission_optim_orders.csv\", index=False)\n",
    "df_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bb0446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44ea43c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9e2d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec24c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb6d964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a35cc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb49c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e6b7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fbc8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7205b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5919034a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf00c0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de2a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb2e4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f0ddda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8e9091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e4f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70d572e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0d4f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d90472a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf90f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae16086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f1a6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd46d467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b32cbce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a0df90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449be061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459129a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29a4ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca3e6df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e59fae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
